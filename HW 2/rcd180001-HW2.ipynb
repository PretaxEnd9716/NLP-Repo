{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import nltk \n",
    "from nltk.book import text1\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.text\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above imports all needed functions from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar']\n"
     ]
    }
   ],
   "source": [
    "tokens = text1.tokens[:20]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above prints out the first 20 tokens of text1. The text object is made out of token objects and tokens are stored as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 455 matches:\n",
      "the sea \n",
      "Indian Sea \n",
      "the sea \n",
      "the sea \n",
      "the sea \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(text1.concordance('sea', 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above prints out the first 5 instances of sea in text1. Concordance method makes a list of specified words. The cell above finds the first 5 lines that contains the word 'sea'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's count method uses python's count method on the text's tokens to find the amount of instances of a given word. They're pretty similar, but with Python's count method you can choose where exactly to look within a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n",
      "433\n"
     ]
    }
   ],
   "source": [
    "print(text1.count('sea'))\n",
    "print(text1.tokens.count('sea'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both lines count the instances of sea in text 1. The first line calls the text's count method. The second line calls the token's count method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['With', 'the', 'All', 'Spark', 'gone', ',', 'we', 'can', 'not', 'return']\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"\"\"With the All Spark gone, we cannot return life to our planet. \n",
    "And fate has yielded its reward: a new world to call... home. \n",
    "We live among its people now, hiding in plain sight... but watching over them in secret... waiting... protecting. \n",
    "I have witnessed their capacity for courage, and though we are worlds apart, like us, there's more to them than meets the eye. \n",
    "I am Optimus Prime, and I send this message to any surviving Autobots taking refuge among the stars: We are here... we are waiting.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw_text)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['With the All Spark gone, we cannot return life to our planet.', 'And fate has yielded its reward: a new world to call... home.', 'We live among its people now, hiding in plain sight... but watching over them in secret... waiting... protecting.', \"I have witnessed their capacity for courage, and though we are worlds apart, like us, there's more to them than meets the eye.\", 'I am Optimus Prime, and I send this message to any surviving Autobots taking refuge among the stars: We are here... we are waiting.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(raw_text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell separates each sentences with NLTK's sentence tokenizer, then prints them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'the', 'all', 'spark', 'gone', ',', 'we', 'can', 'not', 'return', 'life', 'to', 'our', 'planet', '.', 'and', 'fate', 'ha', 'yield', 'it', 'reward', ':', 'a', 'new', 'world', 'to', 'call', '...', 'home', '.', 'we', 'live', 'among', 'it', 'peopl', 'now', ',', 'hide', 'in', 'plain', 'sight', '...', 'but', 'watch', 'over', 'them', 'in', 'secret', '...', 'wait', '...', 'protect', '.', 'i', 'have', 'wit', 'their', 'capac', 'for', 'courag', ',', 'and', 'though', 'we', 'are', 'world', 'apart', ',', 'like', 'us', ',', 'there', \"'s\", 'more', 'to', 'them', 'than', 'meet', 'the', 'eye', '.', 'i', 'am', 'optimu', 'prime', ',', 'and', 'i', 'send', 'thi', 'messag', 'to', 'ani', 'surviv', 'autobot', 'take', 'refug', 'among', 'the', 'star', ':', 'we', 'are', 'here', '...', 'we', 'are', 'wait', '.']\n"
     ]
    }
   ],
   "source": [
    "stemText = [PorterStemmer().stem(t) for t in tokens]\n",
    "print(stemText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above uses the stem function from porter stemmer to remove all the affixes from each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['With', 'the', 'All', 'Spark', 'gone', ',', 'we', 'can', 'not', 'return', 'life', 'to', 'our', 'planet', '.', 'And', 'fate', 'ha', 'yielded', 'it', 'reward', ':', 'a', 'new', 'world', 'to', 'call', '...', 'home', '.', 'We', 'live', 'among', 'it', 'people', 'now', ',', 'hiding', 'in', 'plain', 'sight', '...', 'but', 'watching', 'over', 'them', 'in', 'secret', '...', 'waiting', '...', 'protecting', '.', 'I', 'have', 'witnessed', 'their', 'capacity', 'for', 'courage', ',', 'and', 'though', 'we', 'are', 'world', 'apart', ',', 'like', 'u', ',', 'there', \"'s\", 'more', 'to', 'them', 'than', 'meet', 'the', 'eye', '.', 'I', 'am', 'Optimus', 'Prime', ',', 'and', 'I', 'send', 'this', 'message', 'to', 'any', 'surviving', 'Autobots', 'taking', 'refuge', 'among', 'the', 'star', ':', 'We', 'are', 'here', '...', 'we', 'are', 'waiting', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizedText = [WordNetLemmatizer().lemmatize(t) for t in tokens]\n",
    "print(lemmatizedText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above uses the lemmatize function from the WordNetLemmatizer to convert each tokens to their root form.\n",
    "\n",
    "<p>Stem-Lemma<br>\n",
    "<p>hide-hiding<br>\n",
    "<p>watch-watching<br>\n",
    "<p>wait-waiting<br>\n",
    "<p>protect-protecting<br>\n",
    "<p>wit-witnessed<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK library is very useful and pretty easy to use, but my only complaint is that downloading and importing all the needed functions can be annoying. The code quality is very good from what I've read, the classes are well organized and there are lots of comments from the developers. I can use NLTK to process text from emails to check whether they're important, use to make a simple chatbot, or to use it for text to speech applications. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
