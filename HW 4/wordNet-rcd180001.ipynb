{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet \n",
    "WordNet is a database of nouns, verbs, adjectives, etc. with definitions and examples. It's organized by synonyms sets called synsets that express a concept.\n",
    "\n",
    "## Noun Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('state.n.04'),\n",
       " Synset('country.n.02'),\n",
       " Synset('nation.n.02'),\n",
       " Synset('country.n.04'),\n",
       " Synset('area.n.01')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Noun Synets\n",
    "wn.synsets('country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synsets Methods\n",
    "definition() - Retrieves the word's gloss or definition \\\n",
    "examples() - Gives examples of using the word \\\n",
    "lemmas() - Returns synonyms of the given word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a politically organized body of people under a single government \n",
      "\n",
      "['the state has elected a new president', 'African nations', \"students who had come to the nation's capitol\", \"the country's largest manufacturer\", 'an industrialized land'] \n",
      "\n",
      "[Lemma('state.n.04.state'), Lemma('state.n.04.nation'), Lemma('state.n.04.country'), Lemma('state.n.04.land'), Lemma('state.n.04.commonwealth'), Lemma('state.n.04.res_publica'), Lemma('state.n.04.body_politic')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extract Definitions, Usage, and Lemmas\n",
    "print(wn.synset('state.n.04').definition(), \"\\n\")\n",
    "print(wn.synset('state.n.04').examples(), \"\\n\")\n",
    "print(wn.synset('state.n.04').lemmas(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Traversal\n",
    "As we traverse through more hypernyms, the more broad the nouns become. This means that multiple nouns are hyponyms of a noun such as location. To prevent a synset to store too many relations with other synsets, they're split up into different defintions and parts of speech to make the different relationships. They're also placed in different levels to prevent one synset to have too many relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('administrative_district.n.01'), Synset('district.n.01'), Synset('region.n.03'), Synset('location.n.01'), Synset('object.n.01'), Synset('physical_entity.n.01'), Synset('entity.n.01')] \n",
      "\n",
      "[Synset('african_country.n.01'), Synset('asian_country.n.01'), Synset('banana_republic.n.01'), Synset('buffer_state.n.01'), Synset('european_country.n.01'), Synset('fatherland.n.01'), Synset('kingdom.n.02'), Synset('north_american_country.n.01'), Synset('south_american_country.n.01'), Synset('sultanate.n.01'), Synset('tax_haven.n.01')]\n"
     ]
    }
   ],
   "source": [
    "#Traversal\n",
    "country = wn.synset('country.n.02')\n",
    "hypernyms = lambda s: s.hypernyms()\n",
    "print(list(country.closure(hypernyms)), \"\\n\")\n",
    "\n",
    "location = wn.synset('location.n.01')\n",
    "level = country.hyponyms();\n",
    "print(level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synset Relationships\n",
    "Each synset has hypernyms, hyponyms, meronyms, holonyms, and antonyms. They're used to describe what type of relationship they have with another synset and they're level in relation to another synset. \\\n",
    "Hypernym - A higher level than the another synset and is considered more broad \\\n",
    "Hyponym - A lower level than another synset and is more specific in its definition \\\n",
    "Meronym - A lower level than another synset and the synset is a part of a certain object \\\n",
    "Holonym - A higher level than another synset and the synset is a whole object with different parts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('administrative_district.n.01')]\n",
      "[Synset('african_country.n.01'), Synset('asian_country.n.01'), Synset('banana_republic.n.01'), Synset('buffer_state.n.01'), Synset('european_country.n.01'), Synset('fatherland.n.01'), Synset('kingdom.n.02'), Synset('north_american_country.n.01'), Synset('south_american_country.n.01'), Synset('sultanate.n.01'), Synset('tax_haven.n.01')] \n",
      "\n",
      "[Synset('domain.n.02'), Synset('midland.n.02')]\n",
      "[Synset('department.n.02'), Synset('state.n.01')] \n",
      "\n",
      "[]\n",
      "[] \n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Synset Relationships\n",
    "country = wn.synset('country.n.02')\n",
    "\n",
    "print(country.hypernyms())\n",
    "\n",
    "print(country.hyponyms(), \"\\n\")\n",
    "\n",
    "print(country.part_meronyms())\n",
    "print(country.member_meronyms(), '\\n')\n",
    "\n",
    "print(country.part_holonyms())\n",
    "print(country.member_holonyms(), '\\n')\n",
    "\n",
    "print(country.lemmas()[0].antonyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verb Synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('direct.v.01'),\n",
       " Synset('target.v.01'),\n",
       " Synset('direct.v.03'),\n",
       " Synset('direct.v.04'),\n",
       " Synset('lead.v.01'),\n",
       " Synset('send.v.01'),\n",
       " Synset('aim.v.01'),\n",
       " Synset('conduct.v.02'),\n",
       " Synset('direct.v.09'),\n",
       " Synset('calculate.v.05'),\n",
       " Synset('steer.v.01'),\n",
       " Synset('address.v.03'),\n",
       " Synset('mastermind.v.01'),\n",
       " Synset('direct.a.01'),\n",
       " Synset('direct.s.02'),\n",
       " Synset('direct.a.03'),\n",
       " Synset('lineal.a.01'),\n",
       " Synset('direct.a.05'),\n",
       " Synset('direct.a.06'),\n",
       " Synset('direct.a.07'),\n",
       " Synset('direct.s.08'),\n",
       " Synset('direct.s.09'),\n",
       " Synset('direct.s.10'),\n",
       " Synset('directly.r.01')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verb Synsets\n",
    "wn.synsets(\"direct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verb Hypernyms\n",
    "Verb synsets have relationships between other synsets like nouns. Unlike nouns, there's not top level synset for verbs like 'entity' for nouns. This is due to as the more broader the verb, the more the meaning has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan and direct (a complex undertaking) \n",
      "\n",
      "['he masterminded the robbery'] \n",
      "\n",
      "[Lemma('mastermind.v.01.mastermind'), Lemma('mastermind.v.01.engineer'), Lemma('mastermind.v.01.direct'), Lemma('mastermind.v.01.organize'), Lemma('mastermind.v.01.organise'), Lemma('mastermind.v.01.orchestrate')] \n",
      "\n",
      "[Synset('plan.v.02'), Synset('think.v.03')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mastermind = wn.synset('mastermind.v.01')\n",
    "\n",
    "#Definition, Examples, Lemmas\n",
    "print(wn.synset('mastermind.v.01').definition(), \"\\n\")\n",
    "print(wn.synset('mastermind.v.01').examples(), \"\\n\")\n",
    "print(wn.synset('mastermind.v.01').lemmas(), \"\\n\")\n",
    "\n",
    "#Traversal\n",
    "hypernyms = lambda s: s.hypernyms()\n",
    "print(list(mastermind.closure(hypernyms)), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphy\n",
    "A function that's rules based to obtain the root form of a a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mastermind\n",
      "state\n",
      "emergency\n"
     ]
    }
   ],
   "source": [
    "#Obtain the root form of a word\n",
    "print(wn.morphy('mastermind', wn.VERB))\n",
    "print(wn.morphy('stated', wn.VERB))\n",
    "print(wn.morphy('emergencies', wn.NOUN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity \n",
    "The Wu-Palmer metric measures the similarity of 2 words by comparing their depths and the most specific ancestor. The formula to calculate this metric is \n",
    "$$ 2 \\times \\frac{\\text{depth(most specific ancestor(Word 1, Word 2))}}{\\text{depth(Word 2)} + \\text{depth(Word 1)}} $$\n",
    "\n",
    "The Lesk algrithim is used to determine what kind of word is being used in a sentence or word sense disambiguation. The algorithm uses the context in the sentence and find the most overlapping synset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu-Palmer:  95.23809523809523\n",
      "Wu-Palmer:  100.0\n",
      "Synset('textbook.n.01')\n",
      "Synset('script.n.01')\n"
     ]
    }
   ],
   "source": [
    "#Import\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "book = wn.synset('book.n.01')\n",
    "pamphlet = wn.synset('pamphlet.n.01')\n",
    "direct = wn.synset('direct.v.01')\n",
    "\n",
    "#Wu-Palmer Metric\n",
    "print(\"Wu-Palmer: \", wn.wup_similarity(book, pamphlet) * 100)\n",
    "print(\"Wu-Palmer: \", wn.wup_similarity(direct, direct) * 100)\n",
    "\n",
    "#Lesk Algorithm\n",
    "sentence = ['I', 'tore', 'up', 'my', 'textbook', '.']\n",
    "print(lesk(sentence, 'textbook', 'n'))\n",
    "\n",
    "sentence = ['I', 'tore', 'up', 'my', 'book', '.']\n",
    "print(lesk(sentence, 'book', 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiWordNet\n",
    "SentiWordNet assigns a positivity, negativity, and objectivity scores to a synset. It can be used to be used to analyze the sentiment of sentences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hate.v.01: PosScore=0.0 NegScore=0.75>\n",
      "0.25 \n",
      "\n",
      "<helium.n.01: PosScore=0.0 NegScore=0.0>\n",
      "<be.v.01: PosScore=0.25 NegScore=0.125>\n",
      "<amaze.v.01: PosScore=0.0 NegScore=0.25>\n",
      "<new.a.01: PosScore=0.375 NegScore=0.0>\n",
      "<sponge.n.01: PosScore=0.0 NegScore=0.0>\n",
      "\n",
      "Positive:  0.625\n",
      "Negative:  0.375\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn \n",
    "\n",
    "#Print out hate's sentiment scores\n",
    "hate = swn.senti_synset('hate.v.01')\n",
    "print(hate)\n",
    "print(hate.obj_score(), \"\\n\")\n",
    "\n",
    "#Sentence \n",
    "positive = 0\n",
    "negative = 0\n",
    "sentence = \"He is amazed with this new sponge\"\n",
    "tokens = sentence.split()\n",
    "\n",
    "for t in tokens:\n",
    "    syn_list = list(swn.senti_synsets(t))\n",
    "    if syn_list:\n",
    "        syn = syn_list[0]\n",
    "        positive += syn.pos_score()\n",
    "        negative += syn.neg_score()\n",
    "        print(syn)\n",
    "\n",
    "print(\"\\nPositive: \", positive)\n",
    "print(\"Negative: \", negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The output of the sentiment of the sentence, seems to be accurate. But, I feel like there are some words that seems neutral to have a positive or negative emotion and could throw off the output. These sentiment scores could be useful to find the most important words in a text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation\n",
    "A collocation is a pair of words that frequently appear in a body of a text and where you cannot replace a word with a synonym. They're found by finding bigrams and find which bigrams have high frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n",
      "None\n",
      "\n",
      "p(Chief Magistrate):  0.11904761904761904\n",
      "p(Chief):  0.3333333333333333\n",
      "p(Magistrate):  0.13095238095238096\n",
      "Point-Wise Mutual Information:  1.4474589769712212\n",
      "\n",
      "p(one another):  0.2619047619047619\n",
      "p(one):  7.166666666666667\n",
      "p(another):  0.7976190476190477\n",
      "Point-Wise Mutual Information:  -4.4479598258014175\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import text4\n",
    "import math\n",
    "\n",
    "#Collocations\n",
    "print(text4.collocations())\n",
    "\n",
    "#Calculate mutual information\n",
    "text = ' '.join(text4.tokens)\n",
    "vocab = len(set(text))\n",
    "\n",
    "chiefMagistrate = text.count('Chief Magistrate')/vocab\n",
    "\n",
    "chief = text.count('Chief')/vocab\n",
    "magistrate = text.count('Magistrate')/vocab\n",
    "\n",
    "mutualInfo = math.log2(chiefMagistrate/ (chief * magistrate))\n",
    "\n",
    "print(\"\\np(Chief Magistrate): \", chiefMagistrate)\n",
    "print(\"p(Chief): \", chief)\n",
    "print(\"p(Magistrate): \", magistrate)\n",
    "print(\"Point-Wise Mutual Information: \", mutualInfo)\n",
    "\n",
    "oneAnother = text.count('one another')/vocab\n",
    "\n",
    "one = text.count('one')/vocab\n",
    "another = text.count('another')/vocab\n",
    "\n",
    "mutualInfo = math.log2(oneAnother/ (one * another))\n",
    "\n",
    "print(\"\\np(one another): \", oneAnother)\n",
    "print(\"p(one): \", one)\n",
    "print(\"p(another): \", another)\n",
    "print(\"Point-Wise Mutual Information: \", mutualInfo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first bigram I chose 'Chief Magistrate' seems to be a collocation since you can't replace any of the words and keep the same meaning, the point-wise mutal information (PMI) confirms this since it's positive. The 'of another' bigram is not a bigram since the PMI is negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
